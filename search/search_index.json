{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Projeto de Machine Learning 2022.2 Bem vindo \u00e0 documenta\u00e7\u00e3o do nosso projeto da mat\u00e9ria Machine Learning! Contribuidores: Nicolas Queiroga Paulo Kim Raphael Lahiry Layout Intro C\u00f3digo Part1 Part2 Part3","title":"Projeto de Machine Learning 2022.2"},{"location":"#projeto-de-machine-learning-20222","text":"Bem vindo \u00e0 documenta\u00e7\u00e3o do nosso projeto da mat\u00e9ria Machine Learning! Contribuidores: Nicolas Queiroga Paulo Kim Raphael Lahiry","title":"Projeto de Machine Learning 2022.2"},{"location":"#layout","text":"Intro C\u00f3digo Part1 Part2 Part3","title":"Layout"},{"location":"about/","text":"Edam gradus buxo Sua dederat deduxit dilataque attulerat quem Lorem markdownum dicite conpellat et egit venitis; ubi nolet nostris. Expulsi Aurora, et tamen amante aurigenae innuba carminaque mox minanti venturis sibila. Harenis fuisses Troiam vana, si Isthmon monstra mirantem orbam! Flammae amans Alcyone , arbore vestra amens umbras huic, manus. Turba petis, eloquio animi aut et quoque te Nisus Aeolidae. Vera Eumelique. Murice at at rigent lacteus mota pro nebulas cuncta conripiantque utque insculpunt. Virgineos vibrata et tyranni ingens ictus illa mare iussis induitur tendebat, dignas. Quoque vatis tela asello terrent excutiant tantum, permanet excipiunt secedere, virgo et altera, latravit. Legem carius exemplis Cephesidas arvis huic suoque traxere inplebat miram. Qua brevibus quoque matrem. Sic pallor, ubi ad spe et albenti, sub est Hippotadae ante. Iterum infundere Meritum facile ante induta, sacro irata ambagibus gravis, cum campus ter medeare quaeque incedit Ancaeo postquam capuloque pedum. Ambit submisit femina, saevaque, vices esse spectacula furialibus; pugnat. Habebat telo causa ungula, stimulataque solutis suos deum hinc. Erat musco, pulvere ille ferro et genetrix agmine: iuvenum quisquis umbra; equi hospes decimo transcribere. Acuta Agenorides sparsi ut conubia rogis convaluit senectus propinquae cingentibus proculcat accipitrem pectus oculos unus Theridamas. Viros laetis, vulnere aetas: suis nostra canes haesit exarsit eritque Diomede Helles et melle solus: in. Hospes et omnia coronatae concipit mihi exuvias est procubuere tamen traxit . Adhaesi vult veneno. O arabat lacertos adclivis evocat vectabantur fuerat quid, eripiat non feror solebat Abantiades pocula . Discite mihi pars atris sole perque Telethusa plenum: tetigit minor et vultibus sex Aonias, virginem. Dant sana simul, mihi est diu aequora stellas adversam Unde alto spectat. Mirabantur pharetrae fama. Nec tutela faunique sistetur socer polypus, simulatque neve quem resolvit tua consistere . Dixere in fuerat caput: et mutat, fulicisque vincta trepidumque regis; esse Sibyllae; spissa et relevare. Feroces aestus mundo? Nunc ingratumque nisi tandem tenent gloria contingere via ictus Iovem meis fecissem suae dant ictibus. Timetque protinus redemit moris: aras ita necat imbribus Thermodontiaca expalluit Aethiopesque rexit voce malis bellum, indignave. Labores equos petentem, quem dicta arabat inanes, leve simili peperit clarique aras, antris Calaisque.","title":"Edam gradus buxo"},{"location":"about/#edam-gradus-buxo","text":"","title":"Edam gradus buxo"},{"location":"about/#sua-dederat-deduxit-dilataque-attulerat-quem","text":"Lorem markdownum dicite conpellat et egit venitis; ubi nolet nostris. Expulsi Aurora, et tamen amante aurigenae innuba carminaque mox minanti venturis sibila. Harenis fuisses Troiam vana, si Isthmon monstra mirantem orbam! Flammae amans Alcyone , arbore vestra amens umbras huic, manus. Turba petis, eloquio animi aut et quoque te Nisus Aeolidae. Vera Eumelique. Murice at at rigent lacteus mota pro nebulas cuncta conripiantque utque insculpunt. Virgineos vibrata et tyranni ingens ictus illa mare iussis induitur tendebat, dignas. Quoque vatis tela asello terrent excutiant tantum, permanet excipiunt secedere, virgo et altera, latravit. Legem carius exemplis Cephesidas arvis huic suoque traxere inplebat miram. Qua brevibus quoque matrem. Sic pallor, ubi ad spe et albenti, sub est Hippotadae ante.","title":"Sua dederat deduxit dilataque attulerat quem"},{"location":"about/#iterum-infundere","text":"Meritum facile ante induta, sacro irata ambagibus gravis, cum campus ter medeare quaeque incedit Ancaeo postquam capuloque pedum. Ambit submisit femina, saevaque, vices esse spectacula furialibus; pugnat. Habebat telo causa ungula, stimulataque solutis suos deum hinc. Erat musco, pulvere ille ferro et genetrix agmine: iuvenum quisquis umbra; equi hospes decimo transcribere. Acuta Agenorides sparsi ut conubia rogis convaluit senectus propinquae cingentibus proculcat accipitrem pectus oculos unus Theridamas. Viros laetis, vulnere aetas: suis nostra canes haesit exarsit eritque Diomede Helles et melle solus: in. Hospes et omnia coronatae concipit mihi exuvias est procubuere tamen traxit . Adhaesi vult veneno. O arabat lacertos adclivis evocat vectabantur fuerat quid, eripiat non feror solebat Abantiades pocula . Discite mihi pars atris sole perque Telethusa plenum: tetigit minor et vultibus sex Aonias, virginem. Dant sana simul, mihi est diu aequora stellas adversam Unde alto spectat. Mirabantur pharetrae fama. Nec tutela faunique sistetur socer polypus, simulatque neve quem resolvit tua consistere . Dixere in fuerat caput: et mutat, fulicisque vincta trepidumque regis; esse Sibyllae; spissa et relevare. Feroces aestus mundo? Nunc ingratumque nisi tandem tenent gloria contingere via ictus Iovem meis fecissem suae dant ictibus. Timetque protinus redemit moris: aras ita necat imbribus Thermodontiaca expalluit Aethiopesque rexit voce malis bellum, indignave. Labores equos petentem, quem dicta arabat inanes, leve simili peperit clarique aras, antris Calaisque.","title":"Iterum infundere"},{"location":"code/","text":"class PrepData: def __init__(self, datasets_dir: str = \"datasets\") -> None: tf.get_logger().setLevel(\"ERROR\") self.root_dir = datasets_dir self.annotations_dir = os.path.join(self.root_dir, \"annotations\") self.images_dir = os.path.join(self.root_dir, \"train2014\") self.tfrecords_dir = os.path.join(self.root_dir, \"tfrecords\") self.annotation_file = os.path.join( self.annotations_dir, \"captions_train2014.json\" ) self.train_size = 30000 self.valid_size = 5000 self.captions_per_image = 2 self.images_per_file = 2000 def get_data(self) -> None: # Download caption annotation files if not os.path.exists(self.annotations_dir): annotation_zip = tf.keras.utils.get_file( \"captions.zip\", cache_dir=os.path.abspath(\".\"), origin=\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\", extract=True, ) os.remove(annotation_zip) # Download image files if not os.path.exists(self.images_dir): image_zip = tf.keras.utils.get_file( \"train2014.zip\", cache_dir=os.path.abspath(\".\"), origin=\"http://images.cocodataset.org/zips/train2014.zip\", extract=True, ) os.remove(image_zip) print(\"Dataset is downloaded and extracted successfully.\") def build_coco(self) -> None: with open(self.annotation_file, \"r\") as f: annotations = json.load(f)[\"annotations\"] self.image_path_to_caption = collections.defaultdict(list) for element in annotations: caption = f\"{element['caption'].lower().rstrip('.')}\" image_path = ( self.images_dir + \"/COCO_train2014_\" + \"%012d.jpg\" % (element[\"image_id\"]) ) self.image_path_to_caption[image_path].append(caption) self.image_paths = list(self.image_path_to_caption.keys()) print(f\"Number of images: {len(self.image_paths)}\") self.train_image_paths = self.image_paths[: self.train_size] self.num_train_files = int(np.ceil(self.train_size / self.images_per_file)) self.train_files_prefix = os.path.join(self.tfrecords_dir, \"train\") self.valid_image_paths = self.image_paths[-self.valid_size :] self.num_valid_files = int(np.ceil(self.valid_size / self.images_per_file)) self.valid_files_prefix = os.path.join(self.tfrecords_dir, \"valid\") tf.io.gfile.makedirs(self.tfrecords_dir) class PrepModel: def __init__( self, num_projection_layers: int, projection_dims: int, dropout_rate: float, ) -> None: self.num_projection_layers = num_projection_layers self.projection_dims = projection_dims self.dropout_rate = dropout_rate def create_vision_encoder(self, trainable=False): # Load the pre-trained Xception model to be used as the base encoder. xception = tf.keras.applications.Xception( include_top=False, weights=\"imagenet\", pooling=\"avg\" ) for layer in xception.layers: layer.trainable = trainable inputs = layers.Input(shape=(299, 299, 3), name=\"image_input\") xception_input = tf.keras.applications.xception.preprocess_input(inputs) embeddings = xception(xception_input) outputs = self.project_embeddings(embeddings) return keras.Model(inputs, outputs, name=\"vision_encoder\") def create_text_encoder(self, trainable=False): # Load the BERT preprocessing module. preprocess = hub.KerasLayer( \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\", name=\"text_preprocessing\", ) # Load the pre-trained BERT model to be used as the base encoder. bert = hub.KerasLayer( \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\", name=\"bert\", ) bert.trainable = False inputs = layers.Input(shape=(), dtype=tf.string, name=\"text_input\") bert_inputs = preprocess(inputs) embeddings = bert(bert_inputs)[\"pooled_output\"] outputs = self.project_embeddings(embeddings) return keras.Model(inputs, outputs, name=\"text_encoder\") class DualEncoder(tf.keras.Model): def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs): super(DualEncoder, self).__init__(**kwargs) self.text_encoder = text_encoder self.image_encoder = image_encoder self.temperature = temperature self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\") def call(self, features, training=False): with tf.device(\"/gpu:0\"): caption_embeddings = self.text_encoder( features[\"caption\"], training=training ) with tf.device(\"/gpu:1\"): image_embeddings = self.image_encoder(features[\"image\"], training=training) return caption_embeddings, image_embeddings def compute_loss(self, caption_embeddings, image_embeddings): logits = ( tf.matmul(caption_embeddings, image_embeddings, transpose_b=True) / self.temperature ) images_similarity = tf.matmul( image_embeddings, image_embeddings, transpose_b=True ) captions_similarity = tf.matmul( caption_embeddings, caption_embeddings, transpose_b=True ) targets = tf.keras.activations.softmax( (captions_similarity + images_similarity) / (2 * self.temperature) ) captions_loss = tf.keras.losses.categorical_crossentropy( y_true=targets, y_pred=logits, from_logits=True ) images_loss = tf.keras.losses.categorical_crossentropy( y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True ) return (captions_loss + images_loss) / 2 def train_step(self, features): with tf.GradientTape() as tape: caption_embeddings, image_embeddings = self(features, training=True) loss = self.compute_loss(caption_embeddings, image_embeddings) gradients = tape.gradient(loss, self.trainable_variables) self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)) self.loss_tracker.update_state(loss) return {\"loss\": self.loss_tracker.result()} def test_step(self, features): caption_embeddings, image_embeddings = self(features, training=False) loss = self.compute_loss(caption_embeddings, image_embeddings) self.loss_tracker.update_state(loss) return {\"loss\": self.loss_tracker.result()} def train( pd: PrepData, epochs, batch_size, num_projection_layers=1, projection_dims=256, dropout_rate=0.1, ): # Create vision and text encoders. pm = PrepModel(num_projection_layers, projection_dims, dropout_rate) text_encoder = pm.create_text_encoder() vision_encoder = pm.create_vision_encoder() # Create a dual encoder model. dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=1.0) # Compile the model. dual_encoder.compile( optimizer=tf.keras.optimizers.experimental.AdamW( learning_rate=0.001, weight_decay=0.001 ) ) # set training and validation data. train_image_paths = pd.train_image_paths num_train_files = int(np.ceil(pd.train_size / pd.images_per_file)) train_files_prefix = os.path.join(pd.tfrecords_dir, \"train\") train_example_count = pd.write_data( train_image_paths, num_train_files, train_files_prefix ) print(f\"{train_example_count} training examples were written to tfrecord files.\") valid_image_paths = pd.valid_image_paths num_valid_files = int(np.ceil(pd.valid_size / pd.images_per_file)) valid_files_prefix = os.path.join(pd.tfrecords_dir, \"valid\") valid_example_count = pd.write_data( valid_image_paths, num_valid_files, valid_files_prefix ) print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\") # Start training. print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\") print(f\"Number of examples (caption-image pairs): {train_example_count}\") print(f\"Batch size: {batch_size}\") print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\") # Get TFRecord datasets. train_dataset = pd.get_dataset( os.path.join(pd.tfrecords_dir, \"train-*.tfrecord\"), batch_size ) valid_dataset = pd.get_dataset( os.path.join(pd.tfrecords_dir, \"valid-*.tfrecord\"), batch_size ) # Create a learning rate scheduler callback. reduce_lr = tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.2, patience=3 ) # Create an early stopping callback. early_stopping = tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=5, restore_best_weights=True ) # Train the model. history = dual_encoder.fit( train_dataset, epochs=epochs, validation_data=valid_dataset, callbacks=[reduce_lr, early_stopping], ) print(\"Training completed. Saving vision and text encoders...\") vision_encoder.save(\"vision_encoder\") text_encoder.save(\"text_encoder\") print(\"Models are saved.\") plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"]) plt.ylabel(\"Loss\") plt.xlabel(\"Epoch\") plt.legend([\"train\", \"valid\"], loc=\"upper right\") plt.savefig(\"loss.png\") def load_models(): vision_encoder = tf.keras.models.load_model(\"vision_encoder\") text_encoder = tf.keras.models.load_model(\"text_encoder\") return vision_encoder, text_encoder def generate_image_embeddings(image_paths, vision_encoder, batch_size): print(f\"Generating embeddings for {len(image_paths)} images...\") image_embeddings = vision_encoder.predict( tf.data.Dataset.from_tensor_slices(image_paths) .map(read_image) .batch(batch_size), verbose=1, ) print(f\"Image embeddings shape: {image_embeddings.shape}.\") return image_embeddings def find_matches( image_embeddings, image_paths, text_encoder, queries, k=9, normalize=True ): query_embedding = text_encoder(tf.convert_to_tensor(queries)) if normalize: image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1) query_embedding = tf.math.l2_normalize(query_embedding, axis=1) dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True) results = tf.math.top_k(dot_similarity, k).indices.numpy() return [[image_paths[index] for index in result] for result in results] def run_model( image_embeddings, image_paths, text_encoder, queries, k=9, normalize=True ): matches = find_matches( image_embeddings, image_paths, text_encoder, queries, k, normalize ) plt.figure(figsize=(20, 20)) for i in range(9): ax = plt.subplot(3, 3, i + 1) plt.imshow(plt.imread(matches[0][i])) plt.axis(\"off\") plt.savefig(f\"matches_for_query.png\") print(\"Saving predictions to 'matches_for_query.png'\") return matches def main(batch_size=128, epochs=10): pd = PrepData() pd.get_data() pd.build_coco() if not os.path.exists(\"vision_encoder\") or not os.path.exists(\"text_encoder\"): train(pd=pd, batch_size=batch_size, epochs=epochs) # Load the models. vision_encoder, text_encoder = load_models() image_embeddings = generate_image_embeddings( pd.image_paths, vision_encoder, batch_size ) # Get the queries. queries = [\"kids playing soccer\"] run_model(image_embeddings, pd.image_paths, text_encoder, queries)","title":"C\u00f3digo completo"},{"location":"code_part1/","text":"Parte I - Extra\u00e7\u00e3o dos dados class PrepData: def __init__(self, datasets_dir: str = \"datasets\") -> None: tf.get_logger().setLevel(\"ERROR\") self.root_dir = datasets_dir self.annotations_dir = os.path.join(self.root_dir, \"annotations\") self.images_dir = os.path.join(self.root_dir, \"train2014\") self.tfrecords_dir = os.path.join(self.root_dir, \"tfrecords\") self.annotation_file = os.path.join( self.annotations_dir, \"captions_train2014.json\" ) self.train_size = 30000 self.valid_size = 5000 self.captions_per_image = 2 self.images_per_file = 2000 Acima temos a inicializa\u00e7\u00e3o da classe que iremos utilizar para carregar os dados (fotos e textos) necess\u00e1rios para que o modelo possa ser treinado. A classe possui alguns atributos que ser\u00e3o utilizados para carregar os dados e tamb\u00e9m para salvar os dados em formato TFRecord. def get_data(self) -> None: # Download caption annotation files if not os.path.exists(self.annotations_dir): annotation_zip = tf.keras.utils.get_file( \"captions.zip\", cache_dir=os.path.abspath(\".\"), origin=\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\", extract=True, ) os.remove(annotation_zip) # Download image files if not os.path.exists(self.images_dir): image_zip = tf.keras.utils.get_file( \"train2014.zip\", cache_dir=os.path.abspath(\".\"), origin=\"http://images.cocodataset.org/zips/train2014.zip\", extract=True, ) os.remove(image_zip) print(\"Dataset is downloaded and extracted successfully.\") Aqui temos o m\u00e9todo que ir\u00e1 baixar os dados necess\u00e1rios para o treinamento do modelo. O m\u00e9todo ir\u00e1 verificar se os dados j\u00e1 foram baixados e extra\u00eddos, caso n\u00e3o tenha sido, ir\u00e1 baixar e extrair os dados. def build_coco(self) -> None: with open(self.annotation_file, \"r\") as f: annotations = json.load(f)[\"annotations\"] self.image_path_to_caption = collections.defaultdict(list) for element in annotations: caption = f\"{element['caption'].lower().rstrip('.')}\" image_path = ( self.images_dir + \"/COCO_train2014_\" + \"%012d.jpg\" % (element[\"image_id\"]) ) self.image_path_to_caption[image_path].append(caption) self.image_paths = list(self.image_path_to_caption.keys()) print(f\"Number of images: {len(self.image_paths)}\") self.train_image_paths = self.image_paths[: self.train_size] self.num_train_files = int(np.ceil(self.train_size / self.images_per_file)) self.train_files_prefix = os.path.join(self.tfrecords_dir, \"train\") self.valid_image_paths = self.image_paths[-self.valid_size :] self.num_valid_files = int(np.ceil(self.valid_size / self.images_per_file)) self.valid_files_prefix = os.path.join(self.tfrecords_dir, \"valid\") tf.io.gfile.makedirs(self.tfrecords_dir) Aqui temos o m\u00e9todo que ir\u00e1 construir o dataset do COCO. O m\u00e9todo ir\u00e1 ler o arquivo JSON que cont\u00e9m as informa\u00e7\u00f5es sobre as fotos e os textos que descrevem as fotos. O m\u00e9todo ir\u00e1 construir um dicion\u00e1rio que ir\u00e1 mapear o caminho da foto para o texto que descreve a foto. O m\u00e9todo tamb\u00e9m ir\u00e1 separar os dados em dados de treino e dados de valida\u00e7\u00e3o.","title":"Extra\u00e7\u00e3o"},{"location":"code_part1/#parte-i-extracao-dos-dados","text":"class PrepData: def __init__(self, datasets_dir: str = \"datasets\") -> None: tf.get_logger().setLevel(\"ERROR\") self.root_dir = datasets_dir self.annotations_dir = os.path.join(self.root_dir, \"annotations\") self.images_dir = os.path.join(self.root_dir, \"train2014\") self.tfrecords_dir = os.path.join(self.root_dir, \"tfrecords\") self.annotation_file = os.path.join( self.annotations_dir, \"captions_train2014.json\" ) self.train_size = 30000 self.valid_size = 5000 self.captions_per_image = 2 self.images_per_file = 2000 Acima temos a inicializa\u00e7\u00e3o da classe que iremos utilizar para carregar os dados (fotos e textos) necess\u00e1rios para que o modelo possa ser treinado. A classe possui alguns atributos que ser\u00e3o utilizados para carregar os dados e tamb\u00e9m para salvar os dados em formato TFRecord. def get_data(self) -> None: # Download caption annotation files if not os.path.exists(self.annotations_dir): annotation_zip = tf.keras.utils.get_file( \"captions.zip\", cache_dir=os.path.abspath(\".\"), origin=\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\", extract=True, ) os.remove(annotation_zip) # Download image files if not os.path.exists(self.images_dir): image_zip = tf.keras.utils.get_file( \"train2014.zip\", cache_dir=os.path.abspath(\".\"), origin=\"http://images.cocodataset.org/zips/train2014.zip\", extract=True, ) os.remove(image_zip) print(\"Dataset is downloaded and extracted successfully.\") Aqui temos o m\u00e9todo que ir\u00e1 baixar os dados necess\u00e1rios para o treinamento do modelo. O m\u00e9todo ir\u00e1 verificar se os dados j\u00e1 foram baixados e extra\u00eddos, caso n\u00e3o tenha sido, ir\u00e1 baixar e extrair os dados. def build_coco(self) -> None: with open(self.annotation_file, \"r\") as f: annotations = json.load(f)[\"annotations\"] self.image_path_to_caption = collections.defaultdict(list) for element in annotations: caption = f\"{element['caption'].lower().rstrip('.')}\" image_path = ( self.images_dir + \"/COCO_train2014_\" + \"%012d.jpg\" % (element[\"image_id\"]) ) self.image_path_to_caption[image_path].append(caption) self.image_paths = list(self.image_path_to_caption.keys()) print(f\"Number of images: {len(self.image_paths)}\") self.train_image_paths = self.image_paths[: self.train_size] self.num_train_files = int(np.ceil(self.train_size / self.images_per_file)) self.train_files_prefix = os.path.join(self.tfrecords_dir, \"train\") self.valid_image_paths = self.image_paths[-self.valid_size :] self.num_valid_files = int(np.ceil(self.valid_size / self.images_per_file)) self.valid_files_prefix = os.path.join(self.tfrecords_dir, \"valid\") tf.io.gfile.makedirs(self.tfrecords_dir) Aqui temos o m\u00e9todo que ir\u00e1 construir o dataset do COCO. O m\u00e9todo ir\u00e1 ler o arquivo JSON que cont\u00e9m as informa\u00e7\u00f5es sobre as fotos e os textos que descrevem as fotos. O m\u00e9todo ir\u00e1 construir um dicion\u00e1rio que ir\u00e1 mapear o caminho da foto para o texto que descreve a foto. O m\u00e9todo tamb\u00e9m ir\u00e1 separar os dados em dados de treino e dados de valida\u00e7\u00e3o.","title":"Parte I - Extra\u00e7\u00e3o dos dados"},{"location":"code_part2/","text":"Parte II - Constru\u00e7\u00e3o do modelo class DualEncoder(tf.keras.Model): def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs): super(DualEncoder, self).__init__(**kwargs) self.text_encoder = text_encoder self.image_encoder = image_encoder self.temperature = temperature self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\") Aqui temos uma classe que foi criada para alterar a classe padr\u00e3o do Keras. A classe ir\u00e1 criar um modelo que ir\u00e1 receber como entrada um texto e uma imagem e ir\u00e1 retornar um vetor de embeddings. A classe tamb\u00e9m ir\u00e1 criar um m\u00e9todo para calcular a perda do modelo. def call(self, features, training=False): with tf.device(\"/gpu:0\"): caption_embeddings = self.text_encoder( features[\"caption\"], training=training ) with tf.device(\"/gpu:1\"): image_embeddings = self.image_encoder(features[\"image\"], training=training) return caption_embeddings, image_embeddings Aqui temos o m\u00e9todo que ir\u00e1 calcular o vetor de embeddings. O m\u00e9todo ir\u00e1 receber como entrada um dicion\u00e1rio que cont\u00e9m o texto e a imagem. O m\u00e9todo ir\u00e1 calcular o vetor de embeddings do texto e do vetor de embeddings da imagem e ir\u00e1 retornar os dois vetores de embeddings. def compute_loss(self, caption_embeddings, image_embeddings): logits = ( tf.matmul(caption_embeddings, image_embeddings, transpose_b=True) / self.temperature ) images_similarity = tf.matmul( image_embeddings, image_embeddings, transpose_b=True ) captions_similarity = tf.matmul( caption_embeddings, caption_embeddings, transpose_b=True ) targets = tf.keras.activations.softmax( (captions_similarity + images_similarity) / (2 * self.temperature) ) captions_loss = tf.keras.losses.categorical_crossentropy( y_true=targets, y_pred=logits, from_logits=True ) images_loss = tf.keras.losses.categorical_crossentropy( y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True ) return (captions_loss + images_loss) / 2 Acima temos a fun\u00e7\u00e3o respons\u00e1vel em atribuir as fun\u00e7\u00e3o de perda (loss functions), uma fun\u00e7\u00e3o que mapeia um evento ou valores de uma ou mais vari\u00e1veis \u200b\u200bem um n\u00famero real representando intuitivamente algum \"custo\" associado ao evento. Contru\u00edmos da seguinte maneira: Logits : uma fun\u00e7\u00e3o Logit, tamb\u00e9m conhecida como fun\u00e7\u00e3o log-odds, \u00e9 uma fun\u00e7\u00e3o que representa valores de probabilidade de 0 a 1 e de infinito negativo a infinito. A fun\u00e7\u00e3o \u00e9 inversa \u00e0 fun\u00e7\u00e3o sigm\u00f3ide que limita os valores entre 0 e 1 no eixo Y, em vez do eixo X. Image Similarity : usamos o tf.matmul , que nos a ajudar a realizar uma simples multiplica\u00e7\u00e3o de matriz tensorial 2-D. Captions Similarity : uma tarefa cross-modal que precisa gerar automaticamente senten\u00e7as naturais coerentes para descrever o conte\u00fado da imagem Targets : em particular, usamos a camada de softmax , que converte um vetor de valores em uma distribui\u00e7\u00e3o de probabilidade. Os elementos do vetor de sa\u00edda est\u00e3o no intervalo (0, 1) e somam 1. Ele \u00e9 usada frequentemente usado como ativa\u00e7\u00e3o a camada de uma rede de classifica\u00e7\u00e3o porque o resultado pode ser interpretado como uma distribui\u00e7\u00e3o de probabilidade. Caption Loss : auxiliado pelo categorical_crossentropy do tf.keras.losses , onde calcula a perda de entropia cruzada entre os r\u00f3tulos e as previs\u00f5es. Image Loss : aplica\u00e7\u00e3o do categorical_crossentropy mas no targets e logits . Aqui temos o m\u00e9todo que ir\u00e1 calcular a perda do modelo. O m\u00e9todo ir\u00e1 receber como entrada os vetores de embeddings do texto e da imagem. O m\u00e9todo ir\u00e1 calcular a similaridade entre os vetores de embeddings e ir\u00e1 calcular a perda do modelo. As fun\u00e7\u00f5es que foram utilizadas para calcular a similaridade entre os vetores de embeddings foram: tf.matmul: Multiplica duas matrizes tf.transpose: Transp\u00f5e uma matriz As fun\u00e7\u00f5es que foram utilizadas para calcular a perda do modelo foram: tf.keras.activations.softmax: Calcula a fun\u00e7\u00e3o softmax tf.keras.losses.categorical_crossentropy: Calcula a perda de entropia cruzada categ\u00f3rica def train_step(self, features): with tf.GradientTape() as tape: caption_embeddings, image_embeddings = self(features, training=True) loss = self.compute_loss(caption_embeddings, image_embeddings) gradients = tape.gradient(loss, self.trainable_variables) self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)) self.loss_tracker.update_state(loss) return {\"loss\": self.loss_tracker.result()} def test_step(self, features): caption_embeddings, image_embeddings = self(features, training=False) loss = self.compute_loss(caption_embeddings, image_embeddings) self.loss_tracker.update_state(loss) return {\"loss\": self.loss_tracker.result()} Aqui temos os m\u00e9todos que ir\u00e3o treinar e testar o modelo. O m\u00e9todo de treino ir\u00e1 receber como entrada um dicion\u00e1rio que cont\u00e9m o texto e a imagem. O m\u00e9todo ir\u00e1 calcular o vetor de embeddings do texto e do vetor de embeddings da imagem e ir\u00e1 calcular a perda do modelo. O m\u00e9todo ir\u00e1 calcular os gradientes e ir\u00e1 atualizar os pesos do modelo. O m\u00e9todo ir\u00e1 retornar o valor da perda do modelo. O m\u00e9todo de teste ir\u00e1 receber como entrada um dicion\u00e1rio que cont\u00e9m o texto e a imagem. O m\u00e9todo ir\u00e1 calcular o vetor de embeddings do texto e do vetor de embeddings da imagem e ir\u00e1 calcular a perda do modelo. O m\u00e9todo ir\u00e1 retornar o valor da perda do modelo.","title":"Modelagem"},{"location":"code_part2/#parte-ii-construcao-do-modelo","text":"class DualEncoder(tf.keras.Model): def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs): super(DualEncoder, self).__init__(**kwargs) self.text_encoder = text_encoder self.image_encoder = image_encoder self.temperature = temperature self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\") Aqui temos uma classe que foi criada para alterar a classe padr\u00e3o do Keras. A classe ir\u00e1 criar um modelo que ir\u00e1 receber como entrada um texto e uma imagem e ir\u00e1 retornar um vetor de embeddings. A classe tamb\u00e9m ir\u00e1 criar um m\u00e9todo para calcular a perda do modelo. def call(self, features, training=False): with tf.device(\"/gpu:0\"): caption_embeddings = self.text_encoder( features[\"caption\"], training=training ) with tf.device(\"/gpu:1\"): image_embeddings = self.image_encoder(features[\"image\"], training=training) return caption_embeddings, image_embeddings Aqui temos o m\u00e9todo que ir\u00e1 calcular o vetor de embeddings. O m\u00e9todo ir\u00e1 receber como entrada um dicion\u00e1rio que cont\u00e9m o texto e a imagem. O m\u00e9todo ir\u00e1 calcular o vetor de embeddings do texto e do vetor de embeddings da imagem e ir\u00e1 retornar os dois vetores de embeddings. def compute_loss(self, caption_embeddings, image_embeddings): logits = ( tf.matmul(caption_embeddings, image_embeddings, transpose_b=True) / self.temperature ) images_similarity = tf.matmul( image_embeddings, image_embeddings, transpose_b=True ) captions_similarity = tf.matmul( caption_embeddings, caption_embeddings, transpose_b=True ) targets = tf.keras.activations.softmax( (captions_similarity + images_similarity) / (2 * self.temperature) ) captions_loss = tf.keras.losses.categorical_crossentropy( y_true=targets, y_pred=logits, from_logits=True ) images_loss = tf.keras.losses.categorical_crossentropy( y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True ) return (captions_loss + images_loss) / 2 Acima temos a fun\u00e7\u00e3o respons\u00e1vel em atribuir as fun\u00e7\u00e3o de perda (loss functions), uma fun\u00e7\u00e3o que mapeia um evento ou valores de uma ou mais vari\u00e1veis \u200b\u200bem um n\u00famero real representando intuitivamente algum \"custo\" associado ao evento. Contru\u00edmos da seguinte maneira: Logits : uma fun\u00e7\u00e3o Logit, tamb\u00e9m conhecida como fun\u00e7\u00e3o log-odds, \u00e9 uma fun\u00e7\u00e3o que representa valores de probabilidade de 0 a 1 e de infinito negativo a infinito. A fun\u00e7\u00e3o \u00e9 inversa \u00e0 fun\u00e7\u00e3o sigm\u00f3ide que limita os valores entre 0 e 1 no eixo Y, em vez do eixo X. Image Similarity : usamos o tf.matmul , que nos a ajudar a realizar uma simples multiplica\u00e7\u00e3o de matriz tensorial 2-D. Captions Similarity : uma tarefa cross-modal que precisa gerar automaticamente senten\u00e7as naturais coerentes para descrever o conte\u00fado da imagem Targets : em particular, usamos a camada de softmax , que converte um vetor de valores em uma distribui\u00e7\u00e3o de probabilidade. Os elementos do vetor de sa\u00edda est\u00e3o no intervalo (0, 1) e somam 1. Ele \u00e9 usada frequentemente usado como ativa\u00e7\u00e3o a camada de uma rede de classifica\u00e7\u00e3o porque o resultado pode ser interpretado como uma distribui\u00e7\u00e3o de probabilidade. Caption Loss : auxiliado pelo categorical_crossentropy do tf.keras.losses , onde calcula a perda de entropia cruzada entre os r\u00f3tulos e as previs\u00f5es. Image Loss : aplica\u00e7\u00e3o do categorical_crossentropy mas no targets e logits . Aqui temos o m\u00e9todo que ir\u00e1 calcular a perda do modelo. O m\u00e9todo ir\u00e1 receber como entrada os vetores de embeddings do texto e da imagem. O m\u00e9todo ir\u00e1 calcular a similaridade entre os vetores de embeddings e ir\u00e1 calcular a perda do modelo. As fun\u00e7\u00f5es que foram utilizadas para calcular a similaridade entre os vetores de embeddings foram: tf.matmul: Multiplica duas matrizes tf.transpose: Transp\u00f5e uma matriz As fun\u00e7\u00f5es que foram utilizadas para calcular a perda do modelo foram: tf.keras.activations.softmax: Calcula a fun\u00e7\u00e3o softmax tf.keras.losses.categorical_crossentropy: Calcula a perda de entropia cruzada categ\u00f3rica def train_step(self, features): with tf.GradientTape() as tape: caption_embeddings, image_embeddings = self(features, training=True) loss = self.compute_loss(caption_embeddings, image_embeddings) gradients = tape.gradient(loss, self.trainable_variables) self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)) self.loss_tracker.update_state(loss) return {\"loss\": self.loss_tracker.result()} def test_step(self, features): caption_embeddings, image_embeddings = self(features, training=False) loss = self.compute_loss(caption_embeddings, image_embeddings) self.loss_tracker.update_state(loss) return {\"loss\": self.loss_tracker.result()} Aqui temos os m\u00e9todos que ir\u00e3o treinar e testar o modelo. O m\u00e9todo de treino ir\u00e1 receber como entrada um dicion\u00e1rio que cont\u00e9m o texto e a imagem. O m\u00e9todo ir\u00e1 calcular o vetor de embeddings do texto e do vetor de embeddings da imagem e ir\u00e1 calcular a perda do modelo. O m\u00e9todo ir\u00e1 calcular os gradientes e ir\u00e1 atualizar os pesos do modelo. O m\u00e9todo ir\u00e1 retornar o valor da perda do modelo. O m\u00e9todo de teste ir\u00e1 receber como entrada um dicion\u00e1rio que cont\u00e9m o texto e a imagem. O m\u00e9todo ir\u00e1 calcular o vetor de embeddings do texto e do vetor de embeddings da imagem e ir\u00e1 calcular a perda do modelo. O m\u00e9todo ir\u00e1 retornar o valor da perda do modelo.","title":"Parte II - Constru\u00e7\u00e3o do modelo"},{"location":"code_part3/","text":"Parte III - Treinamento def train( pd: PrepData, epochs, batch_size, num_projection_layers=1, projection_dims=256, dropout_rate=0.1, ): # Create vision and text encoders. pm = PrepModel(num_projection_layers, projection_dims, dropout_rate) text_encoder = pm.create_text_encoder() vision_encoder = pm.create_vision_encoder() # Create a dual encoder model. dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=1.0) # Compile the model. dual_encoder.compile( optimizer=tf.keras.optimizers.experimental.AdamW( learning_rate=0.001, weight_decay=0.001 ) ) # set training and validation data. train_image_paths = pd.train_image_paths num_train_files = int(np.ceil(pd.train_size / pd.images_per_file)) train_files_prefix = os.path.join(pd.tfrecords_dir, \"train\") train_example_count = pd.write_data( train_image_paths, num_train_files, train_files_prefix ) print(f\"{train_example_count} training examples were written to tfrecord files.\") valid_image_paths = pd.valid_image_paths num_valid_files = int(np.ceil(pd.valid_size / pd.images_per_file)) valid_files_prefix = os.path.join(pd.tfrecords_dir, \"valid\") valid_example_count = pd.write_data( valid_image_paths, num_valid_files, valid_files_prefix ) print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\") # Start training. print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\") print(f\"Number of examples (caption-image pairs): {train_example_count}\") print(f\"Batch size: {batch_size}\") print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\") # Get TFRecord datasets. train_dataset = pd.get_dataset( os.path.join(pd.tfrecords_dir, \"train-*.tfrecord\"), batch_size ) valid_dataset = pd.get_dataset( os.path.join(pd.tfrecords_dir, \"valid-*.tfrecord\"), batch_size ) # Create a learning rate scheduler callback. reduce_lr = tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.2, patience=3 ) # Create an early stopping callback. early_stopping = tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=5, restore_best_weights=True ) # Train the model. history = dual_encoder.fit( train_dataset, epochs=epochs, validation_data=valid_dataset, callbacks=[reduce_lr, early_stopping], ) print(\"Training completed. Saving vision and text encoders...\") vision_encoder.save(\"vision_encoder\") text_encoder.save(\"text_encoder\") print(\"Models are saved.\") plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"]) plt.ylabel(\"Loss\") plt.xlabel(\"Epoch\") plt.legend([\"train\", \"valid\"], loc=\"upper right\") plt.savefig(\"loss.png\") Acima temos a fun\u00e7\u00e3o que ir\u00e1 executar por completo a parte de treino do modelo. A fun\u00e7\u00e3o ir\u00e1 receber como entrada o objeto que cont\u00e9m os dados de treino e valida\u00e7\u00e3o, o n\u00famero de \u00e9pocas, o tamanho do batch, o n\u00famero de camadas de proje\u00e7\u00e3o, a dimens\u00e3o das camadas de proje\u00e7\u00e3o e a taxa de dropout. A fun\u00e7\u00e3o ir\u00e1 criar o modelo de texto e o modelo de imagem. A fun\u00e7\u00e3o ir\u00e1 criar o modelo dual encoder. A fun\u00e7\u00e3o ir\u00e1 compilar o modelo dual encoder. A fun\u00e7\u00e3o ir\u00e1 criar os datasets de treino e valida\u00e7\u00e3o. A fun\u00e7\u00e3o ir\u00e1 criar os callbacks de redu\u00e7\u00e3o de taxa de aprendizado e de early stopping. A fun\u00e7\u00e3o ir\u00e1 treinar o modelo dual encoder. A fun\u00e7\u00e3o ir\u00e1 salvar os modelos de texto e imagem. A fun\u00e7\u00e3o ir\u00e1 plotar o gr\u00e1fico da perda do modelo e ir\u00e1 salvar o gr\u00e1fico em um arquivo. Al\u00e9m disso, foi ultizado o AdamW ao inv\u00e9s do Adam . Otimizadores adaptativos como Adam tornaram-se uma escolha padr\u00e3o para treinar redes neurais. No entanto, ao buscar resultados de \u00faltima gera\u00e7\u00e3o, os pesquisadores geralmente preferem a descida do gradiente estoc\u00e1stico (SGD) com momento, porque foi observado que os modelos treinados com Adam tamb\u00e9m n\u00e3o generalizam. def load_models(): vision_encoder = tf.keras.models.load_model(\"vision_encoder\") text_encoder = tf.keras.models.load_model(\"text_encoder\") return vision_encoder, text_encoder def generate_image_embeddings(image_paths, vision_encoder, batch_size): print(f\"Generating embeddings for {len(image_paths)} images...\") image_embeddings = vision_encoder.predict( tf.data.Dataset.from_tensor_slices(image_paths) .map(read_image) .batch(batch_size), verbose=1, ) print(f\"Image embeddings shape: {image_embeddings.shape}.\") return image_embeddings def find_matches( image_embeddings, image_paths, text_encoder, queries, k=9, normalize=True ): query_embedding = text_encoder(tf.convert_to_tensor(queries)) if normalize: image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1) query_embedding = tf.math.l2_normalize(query_embedding, axis=1) dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True) results = tf.math.top_k(dot_similarity, k).indices.numpy() return [[image_paths[index] for index in result] for result in results] def run_model( image_embeddings, image_paths, text_encoder, queries, k=9, normalize=True ): matches = find_matches( image_embeddings, image_paths, text_encoder, queries, k, normalize ) plt.figure(figsize=(20, 20)) for i in range(9): ax = plt.subplot(3, 3, i + 1) plt.imshow(plt.imread(matches[0][i])) plt.axis(\"off\") plt.savefig(f\"matches_for_query.png\") print(\"Saving predictions to 'matches_for_query.png'\") return matches Acima temos as fun\u00e7\u00f5es que ir\u00e3o executar a parte de infer\u00eancia do modelo. A fun\u00e7\u00e3o load_models ir\u00e1 carregar os modelos de texto e imagem. A fun\u00e7\u00e3o generate_image_embeddings ir\u00e1 gerar os embeddings das imagens. A fun\u00e7\u00e3o find_matches ir\u00e1 encontrar as imagens mais similares para cada consulta. A fun\u00e7\u00e3o run_model ir\u00e1 executar o modelo e ir\u00e1 salvar as imagens mais similares para cada consulta em um arquivo. def main(batch_size=128, epochs=10): pd = PrepData() pd.get_data() pd.build_coco() if not os.path.exists(\"vision_encoder\") or not os.path.exists(\"text_encoder\"): train(pd=pd, batch_size=batch_size, epochs=epochs) # Load the models. vision_encoder, text_encoder = load_models() image_embeddings = generate_image_embeddings( pd.image_paths, vision_encoder, batch_size ) # Get the queries. queries = [\"kids playing soccer\"] run_model(image_embeddings, pd.image_paths, text_encoder, queries) Acima temos a fun\u00e7\u00e3o principal do script. A fun\u00e7\u00e3o ir\u00e1 instanciar o objeto PrepData . A fun\u00e7\u00e3o ir\u00e1 executar o m\u00e9todo get_data do objeto PrepData . A fun\u00e7\u00e3o ir\u00e1 executar o m\u00e9todo build_coco do objeto PrepData . A fun\u00e7\u00e3o ir\u00e1 verificar se os modelos de texto e imagem j\u00e1 foram treinados. Caso n\u00e3o tenham sido treinados, a fun\u00e7\u00e3o ir\u00e1 executar a fun\u00e7\u00e3o train . A fun\u00e7\u00e3o ir\u00e1 carregar os modelos de texto e imagem. A fun\u00e7\u00e3o ir\u00e1 gerar os embeddings das imagens. A fun\u00e7\u00e3o ir\u00e1 definir as consultas. A fun\u00e7\u00e3o ir\u00e1 executar o modelo e ir\u00e1 salvar as imagens mais similares para cada consulta em um arquivo.","title":"Treinamento"},{"location":"code_part3/#parte-iii-treinamento","text":"def train( pd: PrepData, epochs, batch_size, num_projection_layers=1, projection_dims=256, dropout_rate=0.1, ): # Create vision and text encoders. pm = PrepModel(num_projection_layers, projection_dims, dropout_rate) text_encoder = pm.create_text_encoder() vision_encoder = pm.create_vision_encoder() # Create a dual encoder model. dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=1.0) # Compile the model. dual_encoder.compile( optimizer=tf.keras.optimizers.experimental.AdamW( learning_rate=0.001, weight_decay=0.001 ) ) # set training and validation data. train_image_paths = pd.train_image_paths num_train_files = int(np.ceil(pd.train_size / pd.images_per_file)) train_files_prefix = os.path.join(pd.tfrecords_dir, \"train\") train_example_count = pd.write_data( train_image_paths, num_train_files, train_files_prefix ) print(f\"{train_example_count} training examples were written to tfrecord files.\") valid_image_paths = pd.valid_image_paths num_valid_files = int(np.ceil(pd.valid_size / pd.images_per_file)) valid_files_prefix = os.path.join(pd.tfrecords_dir, \"valid\") valid_example_count = pd.write_data( valid_image_paths, num_valid_files, valid_files_prefix ) print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\") # Start training. print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\") print(f\"Number of examples (caption-image pairs): {train_example_count}\") print(f\"Batch size: {batch_size}\") print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\") # Get TFRecord datasets. train_dataset = pd.get_dataset( os.path.join(pd.tfrecords_dir, \"train-*.tfrecord\"), batch_size ) valid_dataset = pd.get_dataset( os.path.join(pd.tfrecords_dir, \"valid-*.tfrecord\"), batch_size ) # Create a learning rate scheduler callback. reduce_lr = tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.2, patience=3 ) # Create an early stopping callback. early_stopping = tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=5, restore_best_weights=True ) # Train the model. history = dual_encoder.fit( train_dataset, epochs=epochs, validation_data=valid_dataset, callbacks=[reduce_lr, early_stopping], ) print(\"Training completed. Saving vision and text encoders...\") vision_encoder.save(\"vision_encoder\") text_encoder.save(\"text_encoder\") print(\"Models are saved.\") plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"]) plt.ylabel(\"Loss\") plt.xlabel(\"Epoch\") plt.legend([\"train\", \"valid\"], loc=\"upper right\") plt.savefig(\"loss.png\") Acima temos a fun\u00e7\u00e3o que ir\u00e1 executar por completo a parte de treino do modelo. A fun\u00e7\u00e3o ir\u00e1 receber como entrada o objeto que cont\u00e9m os dados de treino e valida\u00e7\u00e3o, o n\u00famero de \u00e9pocas, o tamanho do batch, o n\u00famero de camadas de proje\u00e7\u00e3o, a dimens\u00e3o das camadas de proje\u00e7\u00e3o e a taxa de dropout. A fun\u00e7\u00e3o ir\u00e1 criar o modelo de texto e o modelo de imagem. A fun\u00e7\u00e3o ir\u00e1 criar o modelo dual encoder. A fun\u00e7\u00e3o ir\u00e1 compilar o modelo dual encoder. A fun\u00e7\u00e3o ir\u00e1 criar os datasets de treino e valida\u00e7\u00e3o. A fun\u00e7\u00e3o ir\u00e1 criar os callbacks de redu\u00e7\u00e3o de taxa de aprendizado e de early stopping. A fun\u00e7\u00e3o ir\u00e1 treinar o modelo dual encoder. A fun\u00e7\u00e3o ir\u00e1 salvar os modelos de texto e imagem. A fun\u00e7\u00e3o ir\u00e1 plotar o gr\u00e1fico da perda do modelo e ir\u00e1 salvar o gr\u00e1fico em um arquivo. Al\u00e9m disso, foi ultizado o AdamW ao inv\u00e9s do Adam . Otimizadores adaptativos como Adam tornaram-se uma escolha padr\u00e3o para treinar redes neurais. No entanto, ao buscar resultados de \u00faltima gera\u00e7\u00e3o, os pesquisadores geralmente preferem a descida do gradiente estoc\u00e1stico (SGD) com momento, porque foi observado que os modelos treinados com Adam tamb\u00e9m n\u00e3o generalizam. def load_models(): vision_encoder = tf.keras.models.load_model(\"vision_encoder\") text_encoder = tf.keras.models.load_model(\"text_encoder\") return vision_encoder, text_encoder def generate_image_embeddings(image_paths, vision_encoder, batch_size): print(f\"Generating embeddings for {len(image_paths)} images...\") image_embeddings = vision_encoder.predict( tf.data.Dataset.from_tensor_slices(image_paths) .map(read_image) .batch(batch_size), verbose=1, ) print(f\"Image embeddings shape: {image_embeddings.shape}.\") return image_embeddings def find_matches( image_embeddings, image_paths, text_encoder, queries, k=9, normalize=True ): query_embedding = text_encoder(tf.convert_to_tensor(queries)) if normalize: image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1) query_embedding = tf.math.l2_normalize(query_embedding, axis=1) dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True) results = tf.math.top_k(dot_similarity, k).indices.numpy() return [[image_paths[index] for index in result] for result in results] def run_model( image_embeddings, image_paths, text_encoder, queries, k=9, normalize=True ): matches = find_matches( image_embeddings, image_paths, text_encoder, queries, k, normalize ) plt.figure(figsize=(20, 20)) for i in range(9): ax = plt.subplot(3, 3, i + 1) plt.imshow(plt.imread(matches[0][i])) plt.axis(\"off\") plt.savefig(f\"matches_for_query.png\") print(\"Saving predictions to 'matches_for_query.png'\") return matches Acima temos as fun\u00e7\u00f5es que ir\u00e3o executar a parte de infer\u00eancia do modelo. A fun\u00e7\u00e3o load_models ir\u00e1 carregar os modelos de texto e imagem. A fun\u00e7\u00e3o generate_image_embeddings ir\u00e1 gerar os embeddings das imagens. A fun\u00e7\u00e3o find_matches ir\u00e1 encontrar as imagens mais similares para cada consulta. A fun\u00e7\u00e3o run_model ir\u00e1 executar o modelo e ir\u00e1 salvar as imagens mais similares para cada consulta em um arquivo. def main(batch_size=128, epochs=10): pd = PrepData() pd.get_data() pd.build_coco() if not os.path.exists(\"vision_encoder\") or not os.path.exists(\"text_encoder\"): train(pd=pd, batch_size=batch_size, epochs=epochs) # Load the models. vision_encoder, text_encoder = load_models() image_embeddings = generate_image_embeddings( pd.image_paths, vision_encoder, batch_size ) # Get the queries. queries = [\"kids playing soccer\"] run_model(image_embeddings, pd.image_paths, text_encoder, queries) Acima temos a fun\u00e7\u00e3o principal do script. A fun\u00e7\u00e3o ir\u00e1 instanciar o objeto PrepData . A fun\u00e7\u00e3o ir\u00e1 executar o m\u00e9todo get_data do objeto PrepData . A fun\u00e7\u00e3o ir\u00e1 executar o m\u00e9todo build_coco do objeto PrepData . A fun\u00e7\u00e3o ir\u00e1 verificar se os modelos de texto e imagem j\u00e1 foram treinados. Caso n\u00e3o tenham sido treinados, a fun\u00e7\u00e3o ir\u00e1 executar a fun\u00e7\u00e3o train . A fun\u00e7\u00e3o ir\u00e1 carregar os modelos de texto e imagem. A fun\u00e7\u00e3o ir\u00e1 gerar os embeddings das imagens. A fun\u00e7\u00e3o ir\u00e1 definir as consultas. A fun\u00e7\u00e3o ir\u00e1 executar o modelo e ir\u00e1 salvar as imagens mais similares para cada consulta em um arquivo.","title":"Parte III - Treinamento"},{"location":"intro/","text":"Projeto de Machine Learning 2022.2 Contexto O Deep Learning (aprendizado profundo) \u00e9 uma t\u00e9cnica de aprendizado de m\u00e1quina que ensina os computadores a fazerem o que \u00e9 natural para os humanos: aprender pelo exemplo. O DL \u00e9 uma tecnologia fundamental por tr\u00e1s dos carros aut\u00f4nomos, permitindo que eles reconhe\u00e7am um sinal de parada ou distingam um pedestre de um poste de luz. \u00c9 a chave para o controle de voz em dispositivos de consumo, como telefones, tablets, TVs e alto-falantes viva-voz, entre outras tecnologias presentes em nosso cotidiano. Tal sub-\u00e1rea do aprendizado de m\u00e1quina est\u00e1 recebendo muita aten\u00e7\u00e3o ultimamente e por um bom motivo: alcan\u00e7ar resultados que antes n\u00e3o eram poss\u00edveis. Embora o deep learning tenha revolucionado a vis\u00e3o computacional, as abordagens atuais t\u00eam v\u00e1rios problemas importantes: Conjuntos de dados de vis\u00e3o t\u00edpicos s\u00e3o trabalhosos e caros para criar enquanto ensinam apenas um conjunto restrito de conceitos visuais Os modelos de vis\u00e3o padr\u00e3o s\u00e3o bons em uma tarefa e apenas uma tarefa e exigem um esfor\u00e7o significativo para se adaptar a uma nova tarefa Modelos com bom desempenho em benchmarks t\u00eam desempenho decepcionantemente ruim em testes de estresse, lan\u00e7ando d\u00favidas sobre toda a abordagem de aprendizado profundo para vis\u00e3o computacional. Motiva\u00e7\u00e3o Para combater essas barreiras, criou-se o CLIP , uma rede neural que pode ser aplicado a qualquer benchmark de classifica\u00e7\u00e3o visual, simplesmente fornecendo os nomes das categorias visuais a serem reconhecidas, semelhantes aos recursos de \u201czero-shot\u201d do GPT-2 e GPT-3. Nosso projeto foi baseado justamente nessa rede neural, com algumas modifica\u00e7\u00f5es que iremos mostrar na aba C\u00f3digo","title":"Intro"},{"location":"intro/#projeto-de-machine-learning-20222","text":"","title":"Projeto de Machine Learning 2022.2"},{"location":"intro/#contexto","text":"O Deep Learning (aprendizado profundo) \u00e9 uma t\u00e9cnica de aprendizado de m\u00e1quina que ensina os computadores a fazerem o que \u00e9 natural para os humanos: aprender pelo exemplo. O DL \u00e9 uma tecnologia fundamental por tr\u00e1s dos carros aut\u00f4nomos, permitindo que eles reconhe\u00e7am um sinal de parada ou distingam um pedestre de um poste de luz. \u00c9 a chave para o controle de voz em dispositivos de consumo, como telefones, tablets, TVs e alto-falantes viva-voz, entre outras tecnologias presentes em nosso cotidiano. Tal sub-\u00e1rea do aprendizado de m\u00e1quina est\u00e1 recebendo muita aten\u00e7\u00e3o ultimamente e por um bom motivo: alcan\u00e7ar resultados que antes n\u00e3o eram poss\u00edveis. Embora o deep learning tenha revolucionado a vis\u00e3o computacional, as abordagens atuais t\u00eam v\u00e1rios problemas importantes: Conjuntos de dados de vis\u00e3o t\u00edpicos s\u00e3o trabalhosos e caros para criar enquanto ensinam apenas um conjunto restrito de conceitos visuais Os modelos de vis\u00e3o padr\u00e3o s\u00e3o bons em uma tarefa e apenas uma tarefa e exigem um esfor\u00e7o significativo para se adaptar a uma nova tarefa Modelos com bom desempenho em benchmarks t\u00eam desempenho decepcionantemente ruim em testes de estresse, lan\u00e7ando d\u00favidas sobre toda a abordagem de aprendizado profundo para vis\u00e3o computacional.","title":"Contexto"},{"location":"intro/#motivacao","text":"Para combater essas barreiras, criou-se o CLIP , uma rede neural que pode ser aplicado a qualquer benchmark de classifica\u00e7\u00e3o visual, simplesmente fornecendo os nomes das categorias visuais a serem reconhecidas, semelhantes aos recursos de \u201czero-shot\u201d do GPT-2 e GPT-3. Nosso projeto foi baseado justamente nessa rede neural, com algumas modifica\u00e7\u00f5es que iremos mostrar na aba C\u00f3digo","title":"Motiva\u00e7\u00e3o"},{"location":"referencias/","text":"Refer\u00eancias Keras Modelo GitHub CLIP CLIP DALL - E 2 DALL-E 2","title":"Refer\u00eancias"},{"location":"referencias/#referencias","text":"","title":"Refer\u00eancias"},{"location":"referencias/#keras","text":"Modelo GitHub","title":"Keras"},{"location":"referencias/#clip","text":"CLIP","title":"CLIP"},{"location":"referencias/#dall-e-2","text":"DALL-E 2","title":"DALL - E  2"}]}